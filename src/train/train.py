from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
import os
import sys
from torchvision import transforms
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, WeightedRandomSampler
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from tqdm import tqdm
import time
import os
import sys
from torchvision import transforms
from torch.cuda.amp import autocast, GradScaler
model_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../model"))

sys.path.append(model_path)
from densenet import DenseNetBC100
from resnet import ResNet34

# ---- æ–°å¢1ï¼šæ•°æ®é¢„å¤„ç†å¢å¼º ----
transform = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])
dataset ='ustc2016'
train_dataset = ImageFolder(f"/root/wzhdesign/traffic_classification/dataset/{dataset}/final_data/train", transform=transform)
test_dataset = ImageFolder(f"/root/wzhdesign/traffic_classification/dataset/{dataset}/final_data/test", transform=transform)
# æ‰“å°ç±»åˆ«æ ‡ç­¾å¯¹åº”å…³ç³»
print("\n=== ç±»åˆ«æ ‡ç­¾å¯¹åº”å…³ç³» ===")
print(f"è®­ç»ƒé›†ç±»åˆ«æ•°é‡: {len(train_dataset.classes)}")
print("æ ‡ç­¾ç´¢å¼• -> ç±»åˆ«åç§°:")
for i, class_name in enumerate(train_dataset.classes):
    print(f"{i} -> {class_name}")

# ç¡®ä¿æµ‹è¯•é›†ç±»åˆ«é¡ºåºç›¸åŒ
print("\næµ‹è¯•é›†ç±»åˆ«é¡ºåºéªŒè¯:")
print(f"æµ‹è¯•é›†ç±»åˆ«æ•°é‡: {len(test_dataset.classes)}")
for i, class_name in enumerate(test_dataset.classes):
    print(f"{i} -> {class_name}")

# æ£€æŸ¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç±»åˆ«æ˜¯å¦ä¸€è‡´
if train_dataset.classes != test_dataset.classes:
    print("\nâš ï¸ è­¦å‘Š: è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç±»åˆ«é¡ºåºä¸ä¸€è‡´ï¼")
    print("è®­ç»ƒé›†ç±»åˆ«:", train_dataset.classes)
    print("æµ‹è¯•é›†ç±»åˆ«:", test_dataset.classes)
else:
    print("\nâœ… è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç±»åˆ«é¡ºåºä¸€è‡´")
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128)

# åˆå§‹åŒ–æ¨¡å‹
# model = DenseNetBC100(num_c=len(train_dataset.classes))
model = ResNet34(num_c=len(train_dataset.classes))
scaler = GradScaler()  # æ··åˆç²¾åº¦ç¼©æ”¾å™¨


# æ£€æŸ¥GPUå¯ç”¨æ€§
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# å°†æ¨¡å‹ç§»è‡³GPU
model = model.to(device)

optimizer = optim.AdamW(  # æ”¹ç”¨AdamW
    model.parameters(),
    lr=0.001,
    weight_decay=0.01     # å¢å¼ºæ­£åˆ™åŒ–
)
criterion = nn.CrossEntropyLoss()

# ---- ä¿®æ”¹4ï¼šåŠ¨æ€å­¦ä¹ ç‡è°ƒåº¦ ----
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='max', 
    factor=0.5,
    patience=2,
    verbose=True
)



def train():
        # ---- ä¿®æ”¹5ï¼šå¢å¼ºçš„è®­ç»ƒå¾ªç¯ ----
    best_val_acc = 0.0
    patience_counter = 0

    # æ—©åœå‚æ•°
    best_val_acc = 0.0
    patience = 10
    no_improve_epochs = 0
    num_epochs = 10

    from tqdm import tqdm
    import time

    # è®­ç»ƒå‚æ•°
    num_epochs = 50
    best_val_acc = 0.0
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0.0
        start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µï¼ˆæ··åˆç²¾åº¦ï¼‰
        for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            
            with autocast():  # æ··åˆç²¾åº¦å‰å‘
                outputs = model(inputs)
                loss = criterion(outputs, labels)
            
            scaler.scale(loss).backward()  # ç¼©æ”¾æ¢¯åº¦
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # æ¢¯åº¦è£å‰ª
            scaler.step(optimizer)
            scaler.update()
            
            epoch_loss += loss.item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        correct = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                correct += (outputs.argmax(1) == labels).sum().item()
        
        val_acc = 100 * correct / len(test_dataset)
        scheduler.step(val_acc)  # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
        
        # æ—©åœæœºåˆ¶
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save(model.state_dict(), f"best_model_{dataset}.pth")
            print(f"ğŸ† æœ€ä½³æ¨¡å‹ä¿å­˜ï¼Œå‡†ç¡®ç‡: {val_acc:.2f}%")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("ğŸ›‘ æ—©åœè§¦å‘")
                break
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        epoch_time = time.time() - start_time
        print(f"Epoch {epoch+1}/{num_epochs} | "
            f"Time: {epoch_time:.1f}s | "
            f"Loss: {epoch_loss/len(train_loader):.4f} | "
            f"Val Acc: {val_acc:.2f}% | "
            f"LR: {optimizer.param_groups[0]['lr']:.1e}")

    print("Training complete!")
    model.load_state_dict(torch.load(f"best_model_{dataset}.pth"))
    model.eval()
def eval():
    import torch
    from sklearn.metrics import classification_report, confusion_matrix
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from tqdm import tqdm

    # æ··æ·†çŸ©é˜µå¯è§†åŒ–
    def plot_confusion_matrix(labels, preds, classes):
        cm = confusion_matrix(labels, preds)
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                    xticklabels=classes, yticklabels=classes)
        plt.xlabel("é¢„æµ‹æ ‡ç­¾")
        plt.ylabel("çœŸå®æ ‡ç­¾")
        plt.title("æ··æ·†çŸ©é˜µ")
        plt.savefig("confusion_matrix.png")  # ä¿å­˜å›¾ç‰‡
        plt.close()
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load(f"best_model_{dataset}.pth"))
    model.eval()

    # åˆå§‹åŒ–å­˜å‚¨å˜é‡
    all_labels = []
    all_preds = []

    # æµ‹è¯•é›†è¯„ä¼°
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="æµ‹è¯•ä¸­"):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            preds = outputs.argmax(dim=1)
            
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))
    print(f"\nğŸ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2f}%")

    # åˆ†ç±»æŠ¥å‘Šï¼ˆç²¾ç¡®ç‡/å¬å›ç‡/F1ï¼‰
    print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(
        all_labels, all_preds, 
        target_names=test_dataset.classes,  # æ›¿æ¢ä¸ºä½ çš„ç±»åˆ«åç§°åˆ—è¡¨
        digits=4
    ))


    #æ··æ·†çŸ©é˜µè¾“å‡º
    print("\nğŸ“Š æ··æ·†çŸ©é˜µ:")
    print(confusion_matrix(all_labels, all_preds))
    #çŸ©é˜µå¯è§†åŒ–
    plot_confusion_matrix(all_labels, all_preds, test_dataset.classes)
if __name__ == '__main__':
    train()
    eval()